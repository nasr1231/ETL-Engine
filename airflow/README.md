# Sales Pipeline - ETL Documentation

## Overview

This pipeline is built using **Apache Airflow** and designed to perform a full ETL process for sales data. It ingests data from CRM and ERP systems, transforms it using **dbt** into structured silver and gold layers, and loads it into **PostgreSQL**.

---

## Project Folder Structure

```bash

‚îú‚îÄ‚îÄ dags/               # Airflow DAGs
‚îú‚îÄ‚îÄ dbt/                # DBT models and configurations
‚îú‚îÄ‚îÄ datasets/           # Raw data files (CSV, JSON, etc.)
‚îú‚îÄ‚îÄ plugins/            # Custom Airflow plugins
‚îú‚îÄ‚îÄ logs/               # Airflow logs
‚îú‚îÄ‚îÄ Dockerfile          # Custom image definition
‚îú‚îÄ‚îÄ docker-compose.yml  # Compose file defining services
‚îú‚îÄ‚îÄ secrets.env         # Env variables for secure credentials
```


## üöÄ How to Run the Project Locally

Follow these steps to spin up the full ETL pipeline environment using Docker and Docker Compose:

### 1Ô∏è‚É£ Clone the Repository

```bash
git clone https://github.com/nasr1231/ETL-Engine.git
```

### 2Ô∏è‚É£ Prepare Environment Variables

Create a `.env` file or `secrets.env` (as referenced in the `docker-compose.yml`) and add your sensitive environment variables:

```env
# secrets.env
AIRFLOW_UID=50000
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow
```

### 3Ô∏è‚É£ Build the Custom Airflow Image

```bash
docker build -t engine-airflow-custom:latest .
```

> üí° **Important:** Make sure your terminal's working directory is the same directory that contains the `Dockerfile`, otherwise the build will fail.

This builds a custom Airflow image that includes:
- dbt-core
- dbt-postgres
- airflow-dbt-python
- psycopg2
- pandas
- git
- flake8

### 4Ô∏è‚É£ Start the Services

```bash
docker-compose up -d
```

This command will:
- Start PostgreSQL and pgAdmin
- Initialize the Airflow metadata DB
- Launch Airflow webserver, scheduler, and worker
- Create the admin user automatically

### 5Ô∏è‚É£ Access the Services

| Tool        | URL                          | Default Credentials               |
|-------------|------------------------------|------------------------------------|
| Airflow     | [http://localhost:8080](http://localhost:8080) | `admin / admin`                   |
| pgAdmin     | [http://localhost:5050](http://localhost:5050) | `engine@admin.com / admin`        |
| Power BI    | Connect via PostgreSQL (host: `localhost`, port: `5432`) |

---

## DAG Structure

```python
dag_id = "sales_pipeline"
description = "ETL Engine"
tags = ['data_ingestion', 'sales']
schedule_interval = None
```

### Task Flow

#### 1. PostgreSQL Connection Test

- Uses `TaskFlow API`
- Ensures connection to the PostgreSQL database is successful before running the pipeline

#### 2. Ingest Data (Task Group)

**CRM Datasets Ingested:**

- `cust_info.csv` ‚Üí `crm_cust_info`
- `prd_info.csv` ‚Üí `crm_prd_info`
- `sales_details.csv` ‚Üí `crm_sales_details`

**ERP Datasets Ingested:**

- `CUST_AZ12.csv` ‚Üí `erp_cust_az12`
- `LOC_A101.csv` ‚Üí `erp_loc_a101`
- `PX_CAT_G1V2.csv` ‚Üí `erp_px_cat_g1v2`

#### 3. Test DBT Resources

- Executes `dbt debug` command to ensure the environment and project configuration are valid

#### 4. Silver Layer Transformations (Task Group)

**dbt models transformed:**
- `crm_cust_info`
- `crm_prd_info`
- `crm_sales_details`
- `erp_customer_info`
- `erp_customer_locations`
- `erp_px_cat`

Each model undergoes:
- `dbt run`
- `dbt test`

#### 5. Gold Layer Load (Task Group)

**dbt models created:**
- `dim_customers`
- `dim_products`
- `dim_dates`
- `fact_sales`

Each model undergoes:
- `dbt run`
- `dbt test`

---

## Operators Used

- `@task` decorator from TaskFlow API
- `BashOperator` for executing dbt CLI commands

---

## Notes

- Secrets are loaded from `.env` file via `dotenv`
- Pipeline is structured using `TaskGroup` for modularity and clarity
- PostgreSQL credentials are handled securely via environment variables

